---
apiVersion: v1
kind: ConfigMap
metadata:
  name: hive-scripts
  labels: {{- include "hive-metastore.labels" . | nindent 4 }}
data:
    entrypoint.sh: |
      #!/bin/bash

      export PATH=${METASTORE_HOME}/bin:$PATH

      function importCert() {
        PEM_FILE=$1
        PASSWORD=$2
        KEYSTORE=$3
        # number of certs in the PEM file
        CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)

        # For every cert in the PEM file, extract it and import into the JKS keystore
        # awk command: step 1, if line is in the desired cert, print the line
        #              step 2, increment counter when last line of cert is found
        for N in $(seq 0 $(($CERTS - 1))); do
          ALIAS="${PEM_FILE%.*}-$N"
          cat $PEM_FILE |
            awk "n==$N { print }; /END CERTIFICATE/ { n++ }" |
            keytool -noprompt -import -trustcacerts \
                    -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
        done
      }

      set -e

      export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
      # if the s3-compatible ca bundle is mounted in, add to the root Java truststore.
      if [ -a /s3-compatible-ca/ca-bundle.crt ]; then
        echo "Adding /s3-compatible-ca/ca-bundle.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /s3-compatible-ca/ca-bundle.crt changeit $JAVA_HOME/lib/security/cacerts
      fi
      # always add the openshift service-ca.crt if it exists
      if [ -a /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt ]; then
        echo "Adding /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt changeit $JAVA_HOME/lib/security/cacerts
      fi

      # add UID to /etc/passwd if missing
      if ! whoami &> /dev/null; then
          if test -w /etc/passwd || stat -c "%a" /etc/passwd | grep -qE '.[267].'; then
              echo "Adding user ${USER_NAME:-hadoop} with current UID $(id -u) to /etc/passwd"
              # Remove existing entry with user first.
              # cannot use sed -i because we do not have permission to write new
              # files into /etc
              sed  "/${USER_NAME:-hadoop}:x/d" /etc/passwd > /tmp/passwd
              # add our user with our current user ID into passwd
              echo "${USER_NAME:-hadoop}:x:$(id -u):0:${USER_NAME:-hadoop} user:${HOME}:/sbin/nologin" >> /tmp/passwd
              # overwrite existing contents with new contents (cannot replace the
              # file due to permissions)
              cat /tmp/passwd > /etc/passwd
              rm /tmp/passwd
          fi
      fi

      # insert S3 bucket URI from env vars
      cat /metastore-config/metastore-site.xml | sed \
        -e "s#XXX_S3ENDPOINT_XXX#${S3_ENDPOINT}#" \
        -e "s#XXX_S3_ACCESS_KEY_XXX#${AWS_ACCESS_KEY_ID}#" \
        -e "s#XXX_S3_SECRET_XXX#${AWS_SECRET_ACCESS_KEY}#" \
        -e "s#XXX_S3_BUCKET_DIR_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" \
        -e "s#XXX_DATABASE_USER_XXX#${DATABASE_USER}#" \
        -e "s#XXX_DATABASE_PASSWORD_XXX#${DATABASE_PASSWORD}#" \
        -e "s#XXX_DATABASE_CONNECT_URL_XXX#postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}?sslmode=${DATABASE_SSLMODE}#" \
      > $METASTORE_HOME/conf/metastore-site.xml

      ln -s -f /metastore-config/metastore-log4j2.properties $METASTORE_HOME/conf/metastore-log4j2.properties
      ln -s -f /metastore-config/metastore-exec-log4j2.properties $METASTORE_HOME/conf/metastore-exec-log4j2.properties
      ln -s -f /metastore-config/jmx-config.yaml $METASTORE_HOME/conf/jmx-config.yaml

      # Set garbage collection settings
      export GC_SETTINGS="-XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${HADOOP_HOME}/logs/heap_dump.bin -XX:+ExitOnOutOfMemoryError -XX:ErrorFile=${HADOOP_HOME}/logs/java_error%p.log"
      export VM_OPTIONS="$VM_OPTIONS -XX:+UseContainerSupport"

      if [ -n "$JVM_INITIAL_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:InitialRAMPercentage=$JVM_INITIAL_RAM_PERCENTAGE"
      fi
      if [ -n "$JVM_MAX_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:MaxRAMPercentage=$JVM_MAX_RAM_PERCENTAGE"
      fi
      if [ -n "$JVM_MIN_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:MinRAMPercentage=$JVM_MIN_RAM_PERCENTAGE"
      fi

      # Set JMX options
      export JMX_OPTIONS="-javaagent:${METASTORE_HOME}/lib/jmx_exporter.jar=9000:${METASTORE_HOME}/conf/jmx-config.yaml"

      # Set garbage collection logs
      export GC_SETTINGS="${GC_SETTINGS} -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_HOME}/logs/gc.log"
      export GC_SETTINGS="${GC_SETTINGS} -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=3M"

      export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar:${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-aws-${HADOOP_VERSION}.jar
      export HIVE_LOGLEVEL="${HIVE_LOGLEVEL:-INFO}"
      export HADOOP_OPTS="${HADOOP_OPTS} ${VM_OPTIONS} ${GC_SETTINGS} ${JMX_OPTIONS}"
      export HIVE_METASTORE_HADOOP_OPTS=" -Dmetastore.log.level=${HIVE_LOGLEVEL} "
      export HIVE_OPTS="${HIVE_OPTS} --hiveconf metastore.root.logger=${HIVE_LOGLEVEL},console "
      export HADOOP_CLIENT_OPTS="$HADOOP_CLIENT_OPTS -Dlog4j2.formatMsgNoLookups=true"

      set +e
      if schematool -dbType postgres -info -verbose; then
          echo "Hive metastore schema verified."
      else
          if schematool -dbType postgres -initSchema -verbose; then
              echo "Hive metastore schema created."
          else
              echo "Error creating hive metastore: $?"
          fi
      fi
      set -e

      start-metastore
